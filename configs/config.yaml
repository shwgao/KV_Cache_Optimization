# -------- Model / Device --------
model:
  model_name: mistralai/Mistral-7B-Instruct-v0.2
  device: cuda:0         
  dtype: auto 
  gpu_mem_util: 0.5

# -------- Retrieval (RAGatouille / ColBERT) --------
retrieval:
  model_id: colbert-ir/colbertv2.0
  dataset_name: pipeline_dataset
  r_text_index_key: r_text_index
  doc_key: doc_id
  question_key: question
  retrieved_key: retrieved_indices
  page_id_key: page_ids
  top_k: 10                 # top-k retrieval per sample

# -------- Prefill Mode --------
# Choose ONE of: full_kv_reuse | cb_fuse | recompute
prefill:
  mode: full_kv_reuse      
  cb:                        
    prefix_prompt: >
      You will be asked a question after reading several passages.
      Please directly answer the question based on the given passages.
      Do NOT repeat the question. The answer should be within 5 words..
      Passages:
    query_prompt: >
      Answer the question directly based on the given passages.
      Do NOT repeat the question. The answer should be within 5 words.
      Question:
    max_new_tokens: 32

# -------- Cache limits & memory budgets --------
cache:
  max_gpu_chunks: 10
  max_cpu_chunks: 1000
  gpu_memory_limit_gb: 40.0
  cpu_memory_limit_gb: 100.0

# -------- KV builder (prefill & placeholders) --------
kv_builder:
  max_samples: 0           # 0 = all
  dump_placements: true   
  save_placeholders: false

# -------- Generation Settings --------
generation:
  max_new_tokens: 32       # Maximum tokens to generate per sample

# -------- Per-Step Scheduler --------
scheduler:
  max_gpu: 10
  max_samples: 0           # 0 = all
  scheduler_interval: 5    # Run heavy scheduler every N tokens
  promote_per_step: 10      # Number of chunks to predict/promote per token
  enable_progress: false
  out_path: scheduler/decoder_trace.json

# -------- Paths (artifacts written under the main --output dir) --------
paths:
  retrieval_json: retrieval/rag_output.json
  kv_summary_json: kv_build/kv_cache_summary.json
